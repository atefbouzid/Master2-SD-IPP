{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c13ac7c",
   "metadata": {},
   "source": [
    "# Lab : Textual data, pre-processing and features\n",
    "\n",
    "**Author: matthieu.labeau@telecom-paris.fr**\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "1. Confront yourself to the difficulties of retrieving and using (annotated) datasets.\n",
    "2. Learn to clean and process textual data. Try several methods to represent this data.\n",
    "3. Visualize these representations, and reflect on their potential usefulness for the dedicated task. \n",
    "\n",
    "\n",
    "## Necessary dependancies\n",
    "\n",
    "We require the usual python package for storing and processing data:\n",
    "- ```numpy```,  \n",
    "- ```pandas```,\n",
    "- ```altair``` (for visualisation)\n",
    "- ```matplotlib``` (for pre-processing)\n",
    "\n",
    "We will take a quick look at *crawling* for data:\n",
    "- ```beautifulsoup```,\n",
    "\n",
    "Besides, we will need the following packages for data pre-processing, representation and visualization:\n",
    "- The Machine Learning API Scikit-learn ```sklearn``` : http://scikit-learn.org/stable/install.html\n",
    "- The Natural Language Toolkit ```nltk```: http://www.nltk.org/install.html\n",
    "- ```gensim```: https://radimrehurek.com/gensim/\n",
    "\n",
    "And to get a taste of modern models:\n",
    "- ```transformers```\n",
    "- And you will need to install other small libraries to make the model run: ```sentencepiece```\n",
    "\n",
    "**You can install all of these libraries via ```pip``` !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efb809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7416b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"obsinfox.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3ba67",
   "metadata": {},
   "source": [
    "Let's look at the categories shown. A lot of labels are given, and there is several annotators for each articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cfe55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL                      object\n",
      "Title                    object\n",
      "Fake News                 int64\n",
      "Places, Dates, People     int64\n",
      "Facts                     int64\n",
      "Opinions                  int64\n",
      "Subjective                int64\n",
      "Reported information      int64\n",
      "Sources Cited             int64\n",
      "False Information         int64\n",
      "Insinuation               int64\n",
      "Exaggeration              int64\n",
      "Offbeat Title             int64\n",
      "Annotator                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(news.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b9688",
   "metadata": {},
   "source": [
    "Understand the structure of the table. Find a solution to get a list of all URLs and titles, without duplicates: \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e8959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 800 entries, 0 to 799\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   URL                    800 non-null    object\n",
      " 1   Title                  800 non-null    object\n",
      " 2   Fake News              800 non-null    int64 \n",
      " 3   Places, Dates, People  800 non-null    int64 \n",
      " 4   Facts                  800 non-null    int64 \n",
      " 5   Opinions               800 non-null    int64 \n",
      " 6   Subjective             800 non-null    int64 \n",
      " 7   Reported information   800 non-null    int64 \n",
      " 8   Sources Cited          800 non-null    int64 \n",
      " 9   False Information      800 non-null    int64 \n",
      " 10  Insinuation            800 non-null    int64 \n",
      " 11  Exaggeration           800 non-null    int64 \n",
      " 12  Offbeat Title          800 non-null    int64 \n",
      " 13  Annotator              800 non-null    object\n",
      "dtypes: int64(11), object(3)\n",
      "memory usage: 87.6+ KB\n"
     ]
    }
   ],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9efe3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = news[\"URL\"].unique().tolist()\n",
    "titles = news[\"Title\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6e55b",
   "metadata": {},
   "source": [
    "You should see that you have $100$ examples - while we have the titles, it will clearly not be enough to find out which label applies. In order to get the text of the articles, we need to use the provided URLs.\n",
    "This is actually something that is quite frequent, usually for certain datasets with copyright or licensing issues, in particular when third parties are forbidden to distribute the textual data themselves. \n",
    "\n",
    "\n",
    "As you can expect, this might cause issues in the long run !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ce3c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c09d4",
   "metadata": {},
   "source": [
    "How to get the text ? Well, we can use a crawler which will take a look at the urls, get the html, and find the text associated to the article thanks to the html parser.\n",
    "Happily, there exists a html tag specifically for this: ```article```. \n",
    "If you would like to know more, take a look at the [beautiful soup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-text). We can propose code which roughly does what we want: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d995ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gives as the page as a BeautifulSoup object\n",
    "# It represents the document as a nested data structure\n",
    "def get_soup(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# This function will look for the text under the tag \"article\"\n",
    "# If there is not, it will get whatever text which is not labelled as 'style' or 'script' and remove empty lines\n",
    "def get_article(url):\n",
    "    item = get_soup(url)\n",
    "    if (item.article == None):\n",
    "        for script in item(['style', 'script']):\n",
    "            script.extract()    \n",
    "        text = item.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        text = '\\n'.join(line for line in lines if line)\n",
    "        return text\n",
    "    else:\n",
    "        return item.article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a47341",
   "metadata": {},
   "source": [
    "Apply this to create a new column ```Text``` to our previously created table. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bcbb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news[\"Text\"] = news[\"URL\"].apply(get_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb2b20",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Look at the results. What are the different issues encountered ? How could we try to solve them ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afc40db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "La relation entre la technologie et la religion\n",
      "\n",
      "Publié le juillet 3, 2022  par  Le Saker Francophone \n",
      "\n",
      "\n",
      "\n",
      "Par Austin Cline − Le 27 fevrier 2019 − Source Learn Religions\n",
      "\n",
      "De nombreux laïcs et non-croyants de toutes sortes ont tendance à considérer la religion et la science comme fondamentalement incompatibles. Cette incompatibilité est également imaginée pour s’étendre à la relation entre la religion et la technologie, puisque la technologie est un produit de la science et que la science ne peut pas aller de l’avant sans la technologie, surtout aujourd’hui. Ainsi, bon nombre d’athées s’émerveillent avec incrédulité du nombre d’ingénieurs qui sont également des créationnistes et du nombre de personnes dans les industries de haute technologie qui affichent de grandes motivations religieuses.\n",
      "\n",
      "Mélanger technologie et religion\n",
      "Pourquoi assistons-nous à un enchantement généralisé pour la technologie et en même temps à une résurgence mondiale du fondamentalisme religieux ? Nous ne devrions pas supposer que la montée des deux est simplement une coïncidence. Au lieu de présumer que l’éducation et la formation derrière la science et la technologie devraient toujours aboutir à plus de scepticisme religieux et même un peu plus d’athéisme, nous devrions nous demander si peut-être les observations empiriques ne confirment pas nos idées.\n",
      "Les athées sont souvent prêts à critiquer les théistes pour ne pas avoir traité des preuves qui ne répondent pas aux attentes, alors ne tombons pas dans ce même piège.\n",
      "Peut-être y a-t-il des impulsions religieuses sous-jacentes à la poussée de la technologie qui a caractérisé la modernité – des impulsions religieuses qui pourraient également affecter les athées laïcs, s’ils ne sont pas suffisamment conscients d’eux-mêmes pour remarquer ce qui se passe. De telles impulsions pourraient empêcher la technologie et la religion d’être incompatibles. Peut-être que la technologie elle-même devient religieuse par elle-même, éliminant ainsi également les incompatibilités.\n",
      "Les deux possibilités doivent être explorées. Les deux se produisent peut-être depuis des centaines d’années, mais les fondements religieux clairs du progrès technologique sont soit ignorés, soit cachés comme des parents embarrassants.\n",
      "L’enthousiasme que tant de gens ont eu pour la technologie est souvent enraciné – parfois sans le savoir – dans les mythes religieux et les rêves anciens. C’est malheureux car la technologie s’est avérée capable de causer de terribles problèmes à l’humanité, et l’une des raisons à cela peut être les impulsions religieuses que les gens ignorent.\n",
      "La technologie, comme la science, est une marque déterminante de la modernité et si l’avenir doit s’améliorer, certaines prémisses élémentaires devront être identifiées, reconnues et, espérons-le, éliminées.\n",
      "Transcendance religieuse et technologique\n",
      "La clé de tout cela est la transcendance. La promesse de transcender la nature, nos corps, nos natures humaines, nos vies, nos morts, notre histoire, etc. est un élément fondamental de la religion qui n’est souvent pas explicitement reconnu. Cela va bien au-delà de la peur commune de la mort et du désir de la surmonter et aboutit à une négation de tout ce que nous sommes dans un effort pour devenir entièrement autre chose.\n",
      "Depuis mille ans dans la culture occidentale, l’avancement des arts mécaniques – la technologie – a été inspiré par de profonds désirs religieux de transcendance et de rédemption. Bien qu’actuellement occultée par le langage et l’idéologie laïques, la résurgence contemporaine de la religion, voire de l’intégrisme, aux côtés et de pair avec la technologie n’est donc pas une aberration mais simplement la réaffirmation d’une tradition oubliée. Si vous ne reconnaissez pas et ne comprenez pas comment la transcendance religieuse et technologique se sont développées ensemble, vous ne pourrez jamais les contrer avec succès – et encore moins reconnaître quand elles pourraient également se développer en vous.\n",
      "Science médiévale et religion médiévale\n",
      "Le projet d’avancement technologique n’est pas un développement récent ; ses racines remontent au Moyen Âge — et c’est là aussi que se développe le lien entre technologie et religion. La technologie en est venue à être identifiée spécifiquement avec la transcendance chrétienne d’un monde de péchés et la rédemption chrétienne d’une nature humaine déchue.\n",
      "Au début de l’ère chrétienne, rien de tel n’était envisagé. Dans La Cité de Dieu il est écrit que « en dehors de l’art surnaturel de vivre dans la vertu et d’atteindre la béatitude immortelle », rien de ce que les humains peuvent faire ne peut offrir une quelconque consolation pour une vie condamnée à la misère. Les arts mécaniques, aussi avancés soient-ils, n’existaient que pour aider les humains déchus et rien de plus. La rédemption et la transcendance ne pouvaient être atteintes que par la Grâce imméritée de Dieu.\n",
      "Cela a commencé à changer au début du Moyen Âge. Bien que la raison en soit incertaine, l’historienne Lynn White a suggéré que l’introduction de la charrue lourde vers la fin du VIIIe siècle en Europe occidentale aurait pu jouer un rôle. Nous sommes habitués à l’idée de l’assujettissement de l’environnement par l’humanité, mais il faut se rappeler que les gens n’ont pas toujours vu les choses de cette façon. Dans la Genèse, l’homme avait reçu la domination sur le monde naturel, mais il avait ensuite péché et l’avait perdu, et devait par la suite gagner sa vie « à la sueur de son front ».\n",
      "Grâce à la technologie, cependant, les humains pourraient regagner une partie de cette domination et accomplir des choses qu’ils n’auraient jamais pu faire seuls. Au lieu que la nature soit toujours un cran au dessus de l’humanité, pour ainsi dire, la relation entre l’humanité et la nature s’est inversée – la capacité de la machine à travailler est devenue la nouvelle norme, permettant aux gens d’exploiter ce qu’ils avaient. La charrue lourde peut ne pas sembler être un gros changement, mais c’était la première et importante étape du processus.\n",
      "Après cela, les machines et les arts mécaniques ont commencé à être représentés dans les illuminations monastique des calendriers, contrairement à l’utilisation antérieure d’images uniquement spirituelles. D’autres illuminations dépeignent des avancées technologiques aidant les armées des justes de Dieu tandis que l’opposition du mal est décrite comme technologiquement inférieure. C’est peut-être ici que nous voyons les prémices de ce changement d’attitude s’installer et que la technologie devient un aspect de la vertu chrétienne.\n",
      "Tout simplement : ce qui était bon et productif dans la vie s’est identifié au système religieux dominant.\n",
      "Sciences monastiques\n",
      "Les principaux moteurs de l’identification de la religion à la technologie étaient les ordres monastiques, pour qui le travail était déjà effectivement une autre forme de prière et de culte. Cela était particulièrement vrai des moines bénédictins. Au VIe siècle, les arts pratiques et le travail manuel étaient enseignés comme éléments vitaux de la dévotion monastique dont le but de tout temps était la poursuite de la perfection ; le travail manuel n’était pas une fin en soi mais était toujours fait pour des raisons spirituelles. Les arts mécaniques – la technologie – s’inscrivaient facilement dans ce programme et étaient donc eux-mêmes investis d’un but spirituel.\n",
      "Il est important de noter que selon la théologie patristique dominante, les humains n’étaient divins que dans leur nature spirituelle. Le corps était déchu et pécheur, donc la rédemption ne pouvait être obtenue qu’en transcendant le corps. La technologie a fourni un moyen d’y parvenir en permettant à un humain d’accomplir bien plus que ce qui était autrement physiquement possible.\n",
      "La technologie a été déclarée par la philosophe carolingienne Erigena (qui a inventé le terme artesmechaniae , arts mécaniques) comme faisant partie de la dotation originelle de Dieu de l’humanité et non comme un produit de notre état déchu ultérieur. Il a écrit que les arts sont « les liens de l’homme avec le Divin, [et] les cultiver un moyen de salut ». Grâce à l’effort et à l’étude, nos pouvoirs d’avant la chute pourraient peut-être être retrouvés et ainsi nous serions bien avancés pour atteindre la perfection et la rédemption.\n",
      "Il serait difficile d’exagérer l’importance de ce changement idéologique. Les arts mécaniques n’étaient plus simplement une nécessité brute pour les humains déchus ; au lieu de cela, ils s’étaient christianisés et investis d’une signification spirituelle qui ne ferait que croître avec le temps.\n",
      "Millénarisme mécanique\n",
      "Le développement du millénarisme dans le christianisme a également eu un impact significatif sur le traitement de la technologie. Pour Augustin, le temps était laborieux et immuable – il n’était que le compte des humains déchus n’allant nulle part, en tout cas, pas de si tôt. Pendant si longtemps, il n’y a eu aucun enregistrement clair et tangible d’aucune sorte de progrès. Le développement technologique a changé tout cela, surtout une fois qu’il a été identifié comme ayant une importance spirituelle. La technologie pouvait, d’une manière que tout le monde voyait et expérimentait de première main, donner l’assurance que l’humanité améliorait sa position dans la vie et réussissait sur la nature.\n",
      "Une mentalité de « nouveau millénarisme » s’est développée, utilisant explicitement les fruits de la technologie. L’histoire humaine a été redéfinie loin du concept d’Augustin du temps pénible et larmoyant et vers une poursuite active : les tentatives d’atteindre la perfection. On ne s’attendait plus à ce que les gens fassent face passivement et aveuglément à une sombre histoire. Au lieu de cela, les gens sont censés travailler consciemment à se perfectionner, en partie grâce à l’utilisation de la technologie.\n",
      "Plus les arts mécaniques se développaient et plus les connaissances augmentaient, plus il semblait que l’humanité se rapprochait de la fin. Christophe Colomb, par exemple, pensait que le monde finirait environ 150 ans après son époque et se considérait même comme jouant un rôle dans l’accomplissement des prophéties de la fin des temps. Il a contribué à la fois à l’élargissement de la technologie marine et au développement des connaissances brutes avec la découverte de nouveaux continents. Les deux étaient considérés par beaucoup comme des jalons importants sur le chemin de la perfection et, par conséquent, de la fin.\n",
      "De cette façon, la technologie devenait partie intégrante de l’eschatologie chrétienne.\n",
      "Science des Lumières et religion des Lumières\n",
      "L’Angleterre et les Lumières ont joué un rôle important dans le développement de la technologie en tant que moyen matériel à des fins spirituelles. La sotériologie (l’étude du salut) et l’eschatologie (l’étude de la fin des temps) étaient des préoccupations courantes dans les milieux savants. La plupart des hommes instruits ont pris très au sérieux la prophétie de Daniel selon laquelle « beaucoup courront çà et là, et la connaissance augmentera » (Daniel 12:4) comme un signe que la fin était proche.\n",
      "Leurs tentatives d’accroître les connaissances sur le monde et d’améliorer la technologie humaine ne faisaient pas partie d’un programme impartial pour simplement en savoir plus sur le monde, mais plutôt pour être actifs dans les attentes millénaristes de l’Apocalypse. La technologie a joué un rôle clé à cet égard en tant que moyen par lequel les humains ont retrouvé la maîtrise du monde naturel promise dans la Genèse, mais que l’humanité a perdue lors de la Chute. Comme l’observe l’historien Charles Webster, « les puritains pensaient sincèrement que chaque étape de la conquête de la nature représentait un pas vers la condition millénaire ».\n",
      "Roger Bacon\n",
      "Une figure importante dans le développement de la science occidentale moderne est Roger Bacon. Pour Bacon, la science signifiait principalement la technologie et les arts mécaniques – pas à des fins ésotériques mais à des fins utilitaires. L’un de ses intérêts était que l’Antéchrist ne soit pas le seul à posséder des outils technologiques dans les batailles apocalyptiques à venir. Bacon a écrit que:\n",
      "L’Antéchrist utilisera ces moyens librement et efficacement, afin qu’il puisse écraser et confondre le pouvoir de ce monde … l’Église devrait envisager l’emploi de ces inventions en raison des périls futurs aux temps de l’Antéchrist qui, avec la grâce de Dieu, serait facile d’obtenir, si les prélats et les princes favorisaient l’étude et enquêtaient sur les secrets de la nature.\n",
      "Bacon croyait également, comme d’autres, que le savoir-faire technologique était un droit de naissance originel de l’humanité qui avait tout simplement été perdu dans la Chute. Écrivant dans son Opus Majus , il a suggéré que les lacunes contemporaines de la compréhension humaine découlent directement du péché originel : « En raison du péché originel et des péchés particuliers de l’individu, une partie de la connaissance a été endommagée, car la raison est aveugle, la mémoire est faible, et ils seront dépravés. »\n",
      "Ainsi, pour Bacon, l’une des premières lumières du rationalisme scientifique, la poursuite de la connaissance et de la technologie avait trois raisons : premièrement, pour que les avantages de la technologie ne soient pas la seule province de l’Antéchrist ; deuxièmement, afin de regagner le pouvoir et les connaissances perdus après la Chute en Éden ; et troisièmement, afin de surmonter les péchés individuels actuels et d’atteindre la perfection spirituelle.\n",
      "Héritage baconien\n",
      "Les successeurs de Bacon dans la science anglaise l’ont suivi de très près dans ces buts. Comme le note Margaret Jacob : « Presque tous les scientifiques ou promoteurs scientifiques anglais importants du XVIIe siècle, de Robert Boyle à Isaac Newton, croyaient au millénarisme qui approchait. » Cela s’accompagnait du désir de retrouver la perfection originelle et la connaissance adamique perdues avec la Chute.\n",
      "La Royal Society a été fondée en 1660 dans le but d’améliorer les connaissances générales et les connaissances pratiques; ses boursiers ont travaillé dans des enquêtes expérimentales et dans les arts mécaniques. Philosophiquement et scientifiquement, les fondateurs ont été fortement influencés par Francis Bacon. John Wilkins, par exemple, a affirmé dans The Beauty of Providence que l’avancement des connaissances scientifiques permettrait à l’humanité de se remettre de la chute.\n",
      "Robert Hooke a écrit que la Royal Society existait « pour tenter de récupérer les arts et inventions autorisés qui sont perdus ». Thomas Sprat était certain que la science était le moyen idéal pour établir « la rédemption de l’homme ». Robert Boyle pensait que les scientifiques avaient une relation spéciale avec Dieu – qu’ils étaient « nés prêtres de la nature » et qu’ils auraient finalement « une bien plus grande connaissance du merveilleux univers de Dieu qu’Adam lui-même n’aurait pu en avoir ».\n",
      "Les francs-maçons en sont une excroissance directe et un excellent exemple. Dans les écrits maçonniques, Dieu est identifié très spécifiquement comme un praticien des arts mécaniques, le plus souvent comme le « Grand Architecte » qui avait « les Sciences Libérales, en particulier la Géométrie, écrites sur son Cœur ». Les membres sont encouragés à pratiquer les mêmes arts scientifiques non seulement pour récupérer les connaissances adamiques perdues, mais aussi pour devenir plus divins. La franc-maçonnerie était un moyen de rédemption et de perfection par la culture de la science et de la technologie.\n",
      "Un héritage particulier de la franc-maçonnerie pour le reste de la société est le développement de l’ingénierie en tant que profession par les francs-maçons en Angleterre. Auguste Comte a écrit sur le rôle que les ingénieurs joueraient dans la reconquête de l’Eden par l’humanité : « l’établissement de la classe des ingénieurs … constituera, sans aucun doute, l’instrument direct et nécessaire d’une coalition entre hommes de science et industriels, par laquelle seule le nouvel ordre social peut commencer. » Comte suggéra qu’eux, le nouveau sacerdoce, imitent les prêtres et les moines en renonçant aux plaisirs de la chair.\n",
      "À ce stade, il convient de noter que dans le récit de la Genèse, la chute se produit lorsqu’Adam et Eve mangent le fruit défendu de la connaissance – la connaissance du bien et du mal. Il est donc ironique de voir des scientifiques promouvoir une augmentation des connaissances dans le but de retrouver la perfection perdue.\n",
      "Science moderne et religion moderne\n",
      "Rien de décrit jusqu’à présent n’est de l’histoire ancienne car l’héritage de la science et de la technologie religieuses reste avec nous. Aujourd’hui, les impulsions religieuses qui sous-tendent le progrès technologique prennent deux formes générales : utiliser des doctrines religieuses explicites, en particulier le christianisme, pour expliquer pourquoi la technologie doit être poursuivie et utiliser des images religieuses de transcendance et de rédemption éloignées des doctrines religieuses traditionnelles mais sans qu’elles perdent tout pouvoir de motivation.\n",
      "Un exemple du premier peut être trouvé dans l’exploration spatiale moderne. Le père de la fusée moderne, Werner Von Braun, a utilisé le millénarisme chrétien pour expliquer son désir d’envoyer des humains dans l’espace. Il a écrit que le monde a été « bouleversé » lorsque Jésus est venu sur terre et que « la même chose peut se reproduire aujourd’hui » en explorant l’espace. La science n’entrait pas en conflit avec sa religion, mais la confirmait plutôt : « Dans cette atteinte du nouveau millénaire par la foi en Jésus-Christ, la science peut être un outil précieux plutôt qu’un obstacle ». Le « millénaire » dont il parlait était la fin des temps.\n",
      "Cette ferveur religieuse a été portée par d’autres dirigeants du programme spatial américain. Jerry Klumas, autrefois ingénieur système chevronné à la NASA, a écrit que le christianisme explicite était normal au centre spatial Johnson et que l’augmentation des connaissances apportées par le programme spatial était un accomplissement de la prophétie susmentionnée de Daniel.\n",
      "Tous les premiers astronautes américains étaient de fervents protestants. Il était courant pour eux de se livrer à des rituels religieux ou à des rêveries lorsqu’ils étaient dans l’espace, et ils ont généralement rapporté que l’expérience du vol spatial avait réaffirmé leur foi religieuse. La première mission habitée sur la lune a retransmis la lecture de la Genèse. Avant même que les astronautes ne débarquent sur la lune, Edwin Aldrin a fait la communion dans la capsule – c’était le premier liquide et le premier aliment mangé sur la lune. Il a rappelé plus tard qu’il voyait la Terre d’un point de vue « physiquement transcendant » et espérait que l’exploration spatiale amènerait les gens à « s’éveiller à nouveau aux dimensions mythiques de l’homme ».\n",
      "Intelligence artificielle\n",
      "La tentative de séparer la pensée de l’esprit humain représente une autre tentative de transcender la condition humaine. Au début, les raisons étaient plus explicitement chrétiennes. Descartes considérait le corps comme une preuve de la «déchéance» de l’humanité plutôt que comme une divinité. La chair s’opposait à la raison et empêchait l’esprit de poursuivre l’intellect pur. Sous son influence, les tentatives ultérieures de créer une « machine à penser » sont devenues des tentatives de séparer « l’esprit » immortel et transcendant de la chair mortelle et déchue.\n",
      "Edward Fredkin, l’un des premiers apôtres et chercheurs dans le domaine de l’intelligence artificielle, est devenu convaincu que son développement était le seul espoir de vaincre les limites et la folie humaines. Selon lui, il était possible de voir le monde comme un « grand ordinateur » et il voulait écrire un « algorithme global » qui, s’il était exécuté méthodiquement, conduirait à la paix et à l’harmonie.\n",
      "Marvin Minsky, qui a dirigé le programme d’IA au MIT, considérait le cerveau humain comme rien de plus qu’une « machine à viande » et le corps comme un « gâchis sanglant de matière organique ». C’était son espoir de réaliser quelque chose de plus et de plus grand – des moyens de transcender ce qu’était son humanité. Le cerveau et le corps étaient, selon lui, facilement remplaçables par des machines. Quand il s’agit de la vie, seul le « mental » est vraiment important et c’est quelque chose qu’il voulait réaliser par la technologie.\n",
      "Il existe des désirs communs parmi les membres de la communauté de l’IA d’utiliser des machines pour transcender leur propre vie : télécharger leur « esprit » dans des machines et peut-être vivre éternellement. Hans Moravec a écrit que les machines intelligentes fourniraient à l’humanité «l’immortalité personnelle par greffe d’esprit» et que ce serait une «défense contre la perte gratuite de connaissances et de fonctions qui est le pire aspect de la mort personnelle».\n",
      "Cyberespace\n",
      "Il n’y a pas assez de temps ou d’espace pour aborder les nombreux thèmes religieux derrière les armes nucléaires ou le génie génétique, mais le développement du cyberespace et d’Internet ne peuvent être ignoré ici. Il ne fait aucun doute que la progression d’Internet dans la vie des gens a un effet profond sur la culture humaine. Que l’on soit technophile qui s’en réjouisse ou néo-luddite qui s’y oppose, tous s’accordent à dire que quelque chose de nouveau prends forme. Beaucoup des premiers considèrent cela comme une forme de salut tandis que les seconds voient cela comme une autre chute.\n",
      "Si vous lisez les écrits de nombreux technophiles qui travaillent dur pour promouvoir l’utilisation du cyberespace, vous ne pouvez qu’être frappé par le mysticisme évident inhérent aux expériences qu’ils tentent de décrire. Karen Armstrong a décrit l’expérience mystique de la communion comme « un sentiment d’unité de toutes choses … le sentiment d’absorption dans une réalité plus grande et ineffable ». Bien qu’elle ait eu à l’esprit les systèmes religieux traditionnels, il convient de se souvenir de cette description lorsque nous examinons des déclarations ostensiblement non religieuses d’apôtres laïcs du cyberespace.\n",
      "John Brockman, éditeur numérique et auteur, a écrit : « Je suis Internet. Je suis le World Wide Web. Je suis l’information. Je suis content ». Michael Heim, consultant et philosophe, a écrit : « Notre fascination pour les ordinateurs… est plus profondément spirituelle qu’utilitaire. Lorsque nous sommes en ligne, nous nous libérons de l’existence corporelle ». Nous imitons alors la « perspective de Dieu », une unicité de la « connaissance divine ». Michael Benedikt écrit : « La réalité, c’est la mort. Si seulement nous le pouvions, nous errerions sur la terre et ne quitterions jamais la maison ; nous jouirions de triomphes sans risques et mangerions de l’Arbre sans être punis, fréquenterions quotidiennement des anges, entrerions au ciel maintenant et non en mourant. »\n",
      "Une fois de plus, nous constatons que la technologie – Internet – est promue comme un moyen d’atteindre la transcendance. Pour certains, il s’agit d’une transcendance religieuse non traditionnelle du corps et des limites matérielles dans le domaine éphémère et ineffable connu sous le nom de « cyberespace ». Pour d’autres, c’est une tentative de transcender nos limites et de réacquérir la divinité personnelle.\n",
      "Technologie et religion\n",
      "Dans d’autres sections, nous avons examiné la question de savoir si oui ou non la science et la technologie étaient réellement incompatibles avec la religion, comme on le pense si souvent. Il semble qu’ils puissent parfois être très compatibles, et en outre que la poursuite du progrès technologique a souvent été le résultat direct de la religion et des aspirations religieuses.\n",
      "Mais ce qui devrait davantage préoccuper les laïcs et les non-croyants, c’est le fait que ces aspirations religieuses ne sont pas toujours de nature religieuse évidente – et si elles ne sont pas si manifestement religieuses au sens traditionnel, on pourrait ne pas reconnaître une impulsion religieuse croissante en eux-mêmes. Parfois, le désir ou la promotion du progrès technologique découle de l’impulsion religieuse fondamentale de transcender l’humanité. Alors que les histoires religieuses traditionnelles et la mythologie (telles que les références chrétiennes explicites à Eden) peuvent avoir disparu depuis, l’impulsion reste fondamentalement religieuse, même lorsqu’elle n’est plus reconnaissable pour ceux qui y sont activement engagés.\n",
      "Pour tous les objectifs de transcendance d’un autre monde, cependant, des puissances très mondaines en ont bénéficié. Les moines bénédictins ont été parmi les premiers à utiliser la technologie comme outil spirituel, mais finalement, leur statut dépendait de leur loyauté envers les rois et les papes – et ainsi le travail a cessé d’être une forme de prière et est devenu un moyen de richesse et d’impôts. Francis Bacon rêvait de rédemption technologique, mais réalisa l’enrichissement de la cour royale et plaça toujours la direction d’un nouvel Eden entre les mains d’une élite aristocratique et scientifique.\n",
      "Le schéma se poursuit aujourd’hui : les développeurs d’armes nucléaires, d’exploration spatiale et d’intelligence artificielle sont peut-être poussés par des désirs religieux, mais ils sont soutenus par le financement militaire et les résultats de leurs efforts sont des gouvernements plus puissants, un statu quo plus pernicieux et une élite prééminente de technocrates.\n",
      "La technologie comme religion\n",
      "La technologie cause des problèmes; il n’y a aucun doute sur ce fait, malgré toutes nos tentatives d’utiliser la technologie pour résoudre nos problèmes. Les gens ne cessent de se demander pourquoi les nouvelles technologies n’ont pas résolu nos problèmes et répondu à nos besoins. peut-être pouvons-nous maintenant suggérer une réponse possible et partielle : ils n’ont jamais été censés le faire.\n",
      "Pour beaucoup, le développement des nouvelles technologies a consisté à transcender complètement les préoccupations mortelles et matérielles. Lorsqu’une idéologie, une religion ou une technologie est poursuivie dans le but d’échapper à la condition humaine où les problèmes et les déceptions sont une réalité de la vie, il ne devrait pas être du tout surprenant que ces problèmes humains ne soient pas vraiment résolus, lorsque les besoins humains ne sont pas entièrement satisfaits et lorsque de nouveaux problèmes apparaissent.\n",
      "C’est en soi un problème fondamental avec la religion et pourquoi la technologie peut être une menace – en particulier lorsqu’elle est poursuivie pour des raisons religieuses. Pour tous les problèmes que nous nous créons, nous seuls pourrons les résoudre — et la technologie sera l’un de nos principaux moyens. Ce qu’il faut, ce n’est pas tant un changement de moyens en abandonnant la technologie, mais un changement d’idéologie en abandonnant le désir égaré de transcender la condition humaine et de s’envoler du monde.\n",
      "Ce ne sera pas facile à faire. Au cours des deux derniers siècles, le développement technologique est devenu inévitable et essentiellement déterministe. L’utilisation et le développement de la technologie ont été retirés des débats politiques et idéologiques. Les objectifs ne sont plus considérés, juste les moyens. On a supposé que le progrès technologique se traduirait automatiquement par une société améliorée – assistez simplement à la course pour installer des ordinateurs dans les écoles sans aucune considération de la façon dont ils seront utilisés, et encore moins toute tentative de déterminer qui paiera pour les techniciens, les mises à niveau, la formation, et la maintenance une fois les ordinateurs achetés. Poser des questions à ce sujet est considéré comme non pertinent – et pire, irrévérencieux.\n",
      "Mais c’est quelque chose que nous, athées et laïcs, en particulier, devons nous demander. Un grand nombre d’entre nous sommes de grands promoteurs de la technologie. La plupart des personnes qui lisent ceci sur Internet sont de grands fans des pouvoirs et des potentiels du cyberespace. Nous avons déjà rejeté les mythologies religieuses traditionnelles comme motivations dans nos vies, mais l’un d’entre nous a-t-il manqué des motivations vers la transcendance héritées dans notre promotion du technologique ? Combien d’athées laïcs qui passent autrement du temps à critiquer la religion sont en fait poussés par une impulsion religieuse non reconnue à transcender l’humanité lorsqu’ils font la promotion de la science ou de la technologie ?\n",
      "Nous devons nous regarder longuement et sérieusement et répondre honnêtement : nous tournons-nous vers la technologie pour échapper à la condition humaine avec tous ses problèmes et ses déceptions ? Ou cherchons-nous plutôt à améliorer la condition humaine, malgré ses défauts et ses imperfections ?\n",
      "Austin Cline\n",
      "Traduit par Johann Oriel du blog La loi de l’UN\n",
      "Sources\n",
      "\n",
      "La religion de la technologie : la divinité de l’homme et l’esprit d’invention . David F. Noble.\n",
      "Dormir avec des extraterrestres : la montée de l’irrationalisme et les périls de la piété . Wendy Kaminer.\n",
      "Technologie, pessimisme et postmodernisme . Edité par Yaron Ezrahi, Everett Mendelsohn et Howard P. Segal.\n",
      "Cyberia : La vie dans les tranchées de l’hyperespace . Douglas Rushkoff.\n",
      "Science médiévale et moderne , tome II. AC Crombie.\n",
      "\n",
      "\n",
      " 5 746\n",
      "       Envoyer l'article en PDF        \n",
      "\n",
      "\t\tCe contenu a été publié dans Intelligence artificielle, Religions, Sciences par Le Saker Francophone. Mettez-le en favori avec son permalien.\n",
      "\t\t\n",
      "\t\t\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news[\"Text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0cc30f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accueil>Politique\n",
      "\n",
      "\n",
      "            Bac 2019 : appel à poursuivre la grève jusqu'au brevet \n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Auteur(s)\n",
      "La rédaction de France-Soir\n",
      "\n",
      "\n",
      "\n",
      "                        Publié le 18 juin 2019 - 09:38\n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "La grève des surveillants pourrait concerner l'ensemble des épreuves du bac 2019.\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "L'intersyndicale d'enseignants qui avait appelé à la grève de la surveillance pour l'épreuve de philosophie du bac a décidé d'étendre la protestation sur toute la durée de l'examen, au moins jusqu'au brevet des collèges. L'épreuve d'histoire géographie de ce mardi 18 devrait donc être également concernée, mais les répercussions sont pour l'instant limitées.La grève  de la surveillance des épreuves du bac 2019 devrait se poursuivre ce mardi pour celle d'histoire-géographie et même pour toutes les autres, peut-être même celles du brevet des collèges. Les syndicats ont en effet appelé à poursuivre le mouvement, qui dans un premier temps devait se concentrer uniquement sur l'épreuve de philosophie de lundi.\"Dans certains lycées, où il y a eu des taux de grévistes très importants, les collègues se sont réunis en assemblée générale et ont souhaité reconduire l'appel à la grève de la surveillance pour demain. D'autres collègues n'ont pas pu faire grève aujourd'hui, puisqu'ils n'étaient pas convoqués, et veulent manifester leur mécontentement eux aussi. À ce titre, on a prolongé le préavis pour leur permettre de faire grève demain\", a fait savoir à Franceinfo Frédérique Rollet, secrétaire générale du SNES-FSU (syndicat majoritaire du secondaire).Voir: Comment tricher au bac risque de vous coûter très cher…L'intersyndicale qui regroupe également la CGT et Sud-Education, et les \"Stylos rouges\", proteste contre la réforme Blanquer qui prévoit notamment une refonte du baccalauréat via le contrôle continu. L'appel à la grève est lancé pour toute la semaine des écrits du bac et même jusqu'au 27 juin, premier jour du brevet et journée d'action inter-fédérale contre la loi Fonction publique. Certains enseignants pourraient également refuser de faire passer les oraux ou de corriger les copies.Toutefois selon le ministère de l'Education, le premier appel à la grève n'a réuni que 6,02% des effectifs. Jean-Michel Blanquer a assuré que l'examen se déroulerait \"normalement\" jugeant que les candidats ne devaient pas \"se préoccuper\" des répercutions d'une grève.Lire aussi:Blanquer: \"le baccalauréat se passera de façon normale\" malgré l'appel à la grève\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "L'article vous a plu ? Il a mobilisé notre rédaction qui ne vit que de vos dons.\n",
      "L'information a un coût, d'autant plus que la concurrence des rédactions subventionnées impose un surcroît de rigueur et de professionnalisme.\n",
      "Avec votre soutien, France-Soir continuera à proposer ses articles gratuitement  car nous pensons que tout le monde doit avoir accès à une information libre et indépendante pour se forger sa propre opinion.\n",
      "Vous êtes la condition sine qua non à notre existence, soutenez-nous pour que France-Soir demeure le média français qui fait s’exprimer les plus légitimes.\n",
      "Si vous le pouvez, soutenez-nous mensuellement, à partir de seulement 1€. Votre impact en faveur d’une presse libre n’en sera que plus fort. Merci.\n",
      "Je fais un don à France-Soir\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Tous les vendredis, recevez la newsletter hebdo de France-Soir\n",
      "Je m’inscris\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "View the discussion thread.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news[\"Text\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b5338",
   "metadata": {},
   "source": [
    "This are issues you might encounter, but **do not solve them now ! You can directly move on to the next part.** Next, we will turn to easier-to-access data to work on cleaning and pre-processing text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c37d0",
   "metadata": {},
   "source": [
    "## II - Symbolic document representations\n",
    "\n",
    "Let's give up on this, and find some data that is directly available... you should note that with a lot of user-generated content coming from websites like twitter and reddit, who have made their policies more severe over the year (data only to be accessed through their API; access then became paying) !\n",
    "\n",
    "Still, we can use this [paper](https://aclanthology.org/2020.lrec-1.175/), which propose tweets annotated for the task of *sexism detection*. The data can be found in their [github](https://github.com/aollagnier/Sexism_Twitter_French/tree/main). Take a look at the description of the categories. \n",
    "\n",
    "We will:\n",
    "1. Clean and pre-process this data as needed.\n",
    "2. Obtain **symbolic** representations for each tweet, and look at their similarities. We will look at several ways of doing so.\n",
    "\n",
    "\n",
    "What might we use these representations for ? \n",
    "- Better understanding the structure of this dataset, how it is organized; how documents compare between themselves. With textual data, this is of course valuable in itself for applied linguistics and social sciences; we can basically look at how words are being used. \n",
    "- Serve as feature representations for other models: this is a first transformation to then feed data into another model to perform a task. \n",
    "\n",
    "What kind of representations will we use ?\n",
    "- Let us begin with something very simple. We will represent documents through **counts of word occurrences**. A very convenient way of doing so is to use a **Bag-of-Words (BoW)** vector, containing the counts of each word (regardless of their order of occurrence) in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a252d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"gathered_tweets_labeled.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9eb06e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0     int64\n",
      "Username      object\n",
      "Tweet         object\n",
      "Category      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweets.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bd68b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>maralpoutine</td>\n",
       "      <td>Cette Pécresse est une belle conasse https://t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>giralou</td>\n",
       "      <td>@Stop_Hidalgo De toute façon elle n’écoute per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>bettynutely</td>\n",
       "      <td>@marietheresero5 Faut VÉRIFIER les sources car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bonhygiene</td>\n",
       "      <td>@mariane_lab Et ne vous plaignez pas Madame Pé...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>FredDu2007</td>\n",
       "      <td>@verity_france Déjà Zemour et Mélenchon n’ont ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Username  \\\n",
       "0           0  maralpoutine   \n",
       "1           1       giralou   \n",
       "2           2   bettynutely   \n",
       "3           3    Bonhygiene   \n",
       "4           4    FredDu2007   \n",
       "\n",
       "                                               Tweet Category  \n",
       "0  Cette Pécresse est une belle conasse https://t...        1  \n",
       "1  @Stop_Hidalgo De toute façon elle n’écoute per...        1  \n",
       "2  @marietheresero5 Faut VÉRIFIER les sources car...        0  \n",
       "3  @mariane_lab Et ne vous plaignez pas Madame Pé...        0  \n",
       "4  @verity_france Déjà Zemour et Mélenchon n’ont ...        1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c65fa3",
   "metadata": {},
   "source": [
    "Instead of working with tables, we will put the data into *lists*; this will make it easier and more convenient to manipulate it at first. Try to understand exactly how we build the following lists, and why ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f15e82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_l = [t for t in tweets['Tweet'].tolist() if len(t) > 0]\n",
    "categories_l = [c for c, t in zip(tweets['Category'].tolist(),tweets['Tweet'].tolist()) if len(t) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ca8ea",
   "metadata": {},
   "source": [
    "### II-1 Pre-processing: Tokenization and cleaning\n",
    "\n",
    "The first thing to do is then to turn each tweet from a string into a list of words. The simplest method is to divide the string according to spaces with the command:\n",
    "``text.split()``\n",
    "\n",
    "But we must also be careful to remove special characters that may not have been cleaned up (such as HTML tags if the data was obtained from web pages). Since we're going to count words, we'll have to build a **list of tokens** appearing in our data. In our case, we'd like to reduce this list and make it uniform (ignore capitalization, punctuation, and the shortest words). \n",
    "\n",
    "\n",
    "We will therefore use a function adapted to our needs - but this is a job that we generally don't need to do ourselves, since there are many tools already adapted to most situations. \n",
    "For text cleansing, there are many scripts, based on different tools (regular expressions, for example) that allow you to prepare data. The division of the text into words and the management of punctuation is handled in a step called **tokenization**; if needed, a python package like NLTK contains many different *tokenizers*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6b33b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_16688\\3084775507.py:20: SyntaxWarning: invalid escape sequence '\\['\n",
      "  REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n"
     ]
    }
   ],
   "source": [
    "# We might want to clean the file with various strategies:\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove words beginning by @ ? (Good choice ?)\n",
    "    # text = re.sub(r'()@\\w+', r'\\1', text)\n",
    "    tokens = text.split()        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05fc9318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@gregtabibian', 'je', 'suis', 'un', 'fils', 'de', 'pute', 'à', 'trottinette', '😂', 'mais', 'pas', 'damalgame', 'on', 'ne', 'vote', 'pas', 'tous', 'hidalgo']\n",
      "['@', 'gregtabibian', 'Je', 'suis', 'un', 'fils', 'de', 'pute', 'à', 'trottinette', '😂', 'Mais', 'pas', \"d'amalgame\", ',', 'on', 'ne', 'vote', 'pas', 'tous', 'Hidalgo', '...']\n"
     ]
    }
   ],
   "source": [
    "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
    "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# Example\n",
    "print(clean_and_tokenize(tweets_l[24]))\n",
    "print(word_tokenize(tweets_l[24]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6cab699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The whole dataset\n",
    "clean_tweets_l = [clean_and_tokenize(t) for t in tweets_l]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30553b",
   "metadata": {},
   "source": [
    "### III-2 Obtaning representations: bag-of-words\n",
    "\n",
    "If we consider the set of all the words appearing in our $T$ training documents, which we note $V$ (the **vocabulary**), we can create an index, which is a bijection associating to each $w$ word an integer, which will be its position in $V$. \n",
    "\n",
    "Thus, for a document extracted from a set of documents containing $|V|$ different words, a BoW representation will be a vector of size $|V|$, whose value at the index of a word $w$ will be its number of occurrences in the document. \n",
    "\n",
    "The next function takes as input a list of documents (again, each in the form of a string) and returns:\n",
    "- A vocabulary that associates, to each word encountered, an index\n",
    "- A matrix, with rows representing documents and columns representing words indexed by the vocabulary. In position $(i,j)$, one should have the number of occurrences of the word $j$ in the document $i$.\n",
    "\n",
    "The vocabulary, which was in the form of a *list* in the previous example, can be returned in the form of a *dictionary* whose keys are the words and values are the indices. Since the vocabulary lists the words in the corpus without worrying about their number of occurrences, it can be built up using a set (in python).\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad831d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts, voc = None):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    n_samples = len(texts)\n",
    "    \n",
    "    # If the vocabulary is not known, we need to build it\n",
    "    if voc == None:\n",
    "        words =set()\n",
    "        for t in texts:\n",
    "            words = words.union(set(clean_and_tokenize(t)))\n",
    "        n_features = len(words)\n",
    "        \n",
    "        vocabulary = dict(zip(words, range(n_features)))\n",
    "        \n",
    "    # If it's given, it's quite easier\n",
    "    else:\n",
    "        vocabulary = voc\n",
    "        n_features = len(voc)\n",
    "    \n",
    "    # Creating the matrix counts\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    # Filling the matrix by iterating over the documents and counting the words\n",
    "    for k, t in enumerate(texts): \n",
    "        for w in clean_and_tokenize(t):\n",
    "            counts[k][vocabulary[w]] += 1.\n",
    "    \n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, bow = count_words(tweets_l)\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2945d",
   "metadata": {},
   "source": [
    "We can also use ```scikit-learn```'s tools for this: the **CountVectorizer** class allows us to obtain the same kind of representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca97f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the vectorizer to the training data\n",
    "vectorizer = CountVectorizer()\n",
    "Bow = vectorizer.fit_transform(tweets_l)\n",
    "bow_a = Bow.toarray()\n",
    "print(bow_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6ac0f",
   "metadata": {},
   "source": [
    "Let's first look at the most frequent words. This will require some simple array manipulation:\n",
    "- Retrieving the sum of all word occurences across documents,\n",
    "- Sorting words according to their frequency,\n",
    "- Plotting an histogram for the top words, using the count as value and the word as legend.\n",
    "\n",
    "How can that influence our pre-processing ? \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = bow.sum(axis = 0)\n",
    "top_words = np.argsort(frequency)[::-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53791d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_voc = {i: w for w, i in voc.items()}\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(range(15), frequency[top_words[:15]])\n",
    "ax.set_xticks(range(15))\n",
    "ax.set_xticklabels([rev_voc[i] for i in top_words[:15]], rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6a1a7",
   "metadata": {},
   "source": [
    "### II-3 Vector comparison\n",
    "\n",
    "We can use these very large-dimensional vectors for a very simple semantic analysis: for example, by looking for the nearest neighbors of a tweet. \n",
    "However, we need to be careful to the distance that we use: should it be *Euclidean* or *Cosine*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ec427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(u, v):\n",
    "    return np.linalg.norm(u-v)\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / np.sqrt(u.dot(u))\n",
    "\n",
    "def cosine(u, v):\n",
    "    return 1.0 - length_norm(u).dot(length_norm(v))\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b1a3d",
   "metadata": {},
   "source": [
    "Implement a function using the ```NearestNeighbors``` class for ```sklearn```, allowing you to print the closest document of a reference index. Try both distances and both representations.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_neighbors(distance, texts, representations, index, k=5):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance : function\n",
    "        The distance to use to compare documents\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    representations: 2D Array\n",
    "        Vector representations of the texts, in the same order\n",
    "    index: int\n",
    "        Index of the document for which to return nearest neighbors\n",
    "    k: int\n",
    "        Number of neighbors to display    \n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    print(\"Plus proches voisins de: \\n '%s' \\n selon la distance '%s':\" % (...))\n",
    "    print(...)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neighbors(euclidean, tweets_l, bow, 24)\n",
    "print_neighbors(cosine, tweets_l, bow, 24)\n",
    "\n",
    "print_neighbors(euclidean, tweets_l, bow_a, 24)\n",
    "print_neighbors(cosine, tweets_l, bow_a, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d7d18b",
   "metadata": {},
   "source": [
    "### II-4 Improving representations with TF-IDF\n",
    "\n",
    "This is the product of the frequency of the term (TF) and its inverse frequency in documents (IDF).\n",
    "This method is usually used to measure the importance of a term $i$ in a document $j$ relative to the rest of the corpus, from a matrix of occurrences $ words \\times documents$. Thus, for a matrix $\\mathbf{T}$ of $|V|$ terms and $D$ documents:\n",
    "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n",
    "\n",
    "$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
    "\n",
    "TF-IDF is generally better suited to low-density matrices, since it will penalize terms that appear in a large part of the documents. \n",
    "\n",
    "Implement a function transforming the BOW representations we obtained as output of ```count_words``` into TF-IDF representations. Do not forget about **smoothing** ! \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def tfidf_transform(bow):\n",
    "    \"\"\"\n",
    "    Inverse document frequencies applied to our bag-of-words representations\n",
    "    \"\"\"\n",
    "    # IDF\n",
    "    d = ...\n",
    "    in_doc = ...\n",
    "    idf = ...\n",
    "    # TF\n",
    "    sum_vec = ...\n",
    "    tf = ...\n",
    "    tf_idf = ...\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_transform(bow)\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27bc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the vectorizer to the training data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "Tfidf = tfidf_vectorizer.fit_transform(tweets_l)\n",
    "tfidf_a = Tfidf.toarray()\n",
    "print(tfidf_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa855b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neighbors(euclidean, tweets_l, tfidf_a, 24)\n",
    "print_neighbors(cosine, tweets_l, tfidf_a, 24)\n",
    "\n",
    "print_neighbors(euclidean, tweets_l, tfidf, 24)\n",
    "print_neighbors(cosine, tweets_l, tfidf, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9681f1",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Try to interpret the nearest neighbors for each distance - think about how they are computed. Which kind of documents are privileged, for each of them ? How might the representation affect this ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38f90a",
   "metadata": {},
   "source": [
    "## III - Data visualization\n",
    "\n",
    "**Question:** can these representations capture what the dataset is about ? We will use visualization to find out in this last section. More precisely, we would like to check if lexical features (which we tried to rid of the influence of word frequency with TF-IDF), are enough to capture the characteristics of the dataset (which are reflected in the **annotations**.\n",
    "\n",
    "We will not use supervised Machine Learning yet, but rather try to visualize our data in 2D: it's all about reducing the dimension with techniques that keep what's (statistically) more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784b863",
   "metadata": {},
   "source": [
    "### III - 1 With PCA\n",
    "\n",
    "We will now use **principal components analysis** (PCA) to visualize our data in two dimensions. This is equivalent to applying SVD to the covariance matrix of the data, in order for the principal components to be independant from each other an maximize the variance of the data. We use the class ```PCA``` from ```scikit-learn```.\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "                        \n",
    "Working on TF-IDF representations and displaying classes: \n",
    "- What can we observe ?\n",
    "- How can we interpret this with respect to our features ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "docs_pca = pca.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05441ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x': docs_pca[:,0],\n",
    "                     'y': docs_pca[:,1],\n",
    "                     'tweet': tweets_l,\n",
    "                     'Category': categories_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e086ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(data[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='Category',\n",
    "    tooltip=['tweet']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea44c3",
   "metadata": {},
   "source": [
    "### III - 2 With T-SNE\n",
    "\n",
    "From the ```sklearn``` documentation: \n",
    "- t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. **with different initializations we can get different results**.\n",
    "- In particular, t-SNE has the advantage to reveal data that lie in multiple, different, manifolds or clusters.\n",
    "- It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples.\n",
    "\n",
    "From this recommendation, we will initialize ```TSNE``` with PCA (choosing the argument ```init='pca'``` when creating the class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f51222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb2f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='pca').fit_transform(tfidf)\n",
    "print(docs_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab325aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x': docs_tsne[:,0],\n",
    "                     'y': docs_tsne[:,1],\n",
    "                     'tweet': tweets_l,\n",
    "                     'Category': categories_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(data[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='Category',\n",
    "    tooltip=['tweet']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f9d88",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "                        \n",
    "- Is there any conclusion we can draw with respect to the lexical features and how they allow us to group the documents in this dataset ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c7f96",
   "metadata": {},
   "source": [
    "### III - 3 Topic modeling\n",
    "\n",
    "Now, the goal is to re-use the bag-of-words representations we obtained earlier - but reduce their dimension before visualization. \n",
    "\n",
    "The underlying idea is to **take advantage of the latent structure in the association between the set of\n",
    "words and the set of documents**. Many methods have been designed to do this - the earliest being **topic models**. \n",
    "\n",
    "Note that this allows to obtain reduced document representations, in a **topic space, common to documents and words** - where each document is described as a vector of topics and for each topic, we have access to the importance of words. \n",
    "\n",
    "\n",
    "We will do this with two models:\n",
    "- Using the ```TruncatedSVD```, we will **linearly** reduce the dimension of our BOW representations. This is called *Latent Semantic Analysis* (LSA). \n",
    "- Using a *generative model* based on several assumptions on how a document is generated through topics, which the model will retrieve: this is ```LatentDirichletAllocation``` (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b165899",
   "metadata": {},
   "source": [
    "We use here another dataset from this [paper](https://aclanthology.org/2024.latechclfl-1.28/) which includes quite more categories and will be more interesting to explore, as we can expect it to contain clusters clearly visible through looking at lexical features. You can find the dataset on their [git repository](https://git.unistra.fr/thealtres/stage-direction-classif-french-transfer-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cec8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "AS13_df = pd.read_table(\"stgdir_labelGeneric.csv\",\n",
    "                        sep='|', \n",
    "                        dtype={'description' : 'object', 'labelGeneric': 'category', })\n",
    "labelCol = 'labelGeneric'\n",
    "class_names = sorted(AS13_df[labelCol].unique().categories.to_list())\n",
    "label2id = {class_names[i]:i for i in range(len(class_names))}\n",
    "id2label = {i:class_names[i] for i in range(len(class_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "AS13_df = AS13_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad1373",
   "metadata": {},
   "source": [
    "First, apply the same pipeline than before:\n",
    "- Does the data need to be cleaned and pre-processed ?\n",
    "- Obtain BOW and TF-IDF representations.\n",
    "- Visualize them with T-SNE.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain BOW representations\n",
    "...\n",
    "voc_th, bow_th = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform them into TF-IDF\n",
    "...\n",
    "tfidf_th = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8582599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result of dimension reduction via T-SNE; display the classes\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230bad2",
   "metadata": {},
   "source": [
    "**Latent Semantic Analysis**: let us choose an arbitrary number of topics - which will be the size of the joint *topic space*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac15c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 50\n",
    "lsa = TruncatedSVD(n_components = n_topics)\n",
    "lsa_topics = lsa.fit_transform(tfidf_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspondances between documents and topics\n",
    "print(lsa_topics.shape)\n",
    "# Correspondances between topics and words\n",
    "print(lsa.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversing the vocabulary to retrieve words from indexes, allowing to find the most important words for each topic\n",
    "rev_voc_th = {i: w for w, i in voc_th.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_important_words(n, reverse_vocabulary, topic_model):\n",
    "    out = []\n",
    "    for i, topic in enumerate(topic_model.components_):\n",
    "        out.append([reverse_vocabulary[j] for j in topic.argsort()[:-n-1:-1]])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = most_important_words(8, rev_voc_th, lsa)\n",
    "for i, topic in enumerate(words[:15]):\n",
    "    print(\"Topic \", i+1, \" : \", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504ffd3",
   "metadata": {},
   "source": [
    "With a dataset this size, over **short texts**, it is difficult to interpret the topics (many short words, even with TF-IDF). Let's apply T-SNE ! \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78923ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result of dimension reduction via T-SNE; display the classes\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f952cf5",
   "metadata": {},
   "source": [
    "**Latent Dirichlet Allocation**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = n_topics)\n",
    "lda_topics_th = lda.fit_transform(bow_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = most_important_words(8, rev_voc_th, lda)\n",
    "for i, topic in enumerate(words[:15]):\n",
    "    print(\"Topic \", i+1, \" : \", topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f97120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_topics_th.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94928bb8",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result of dimension reduction via T-SNE; display the classes\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964469f9",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "- Apply the pipeline to obtain a t-sne visualisation over the same representations, but for the **second dataset** (*gathered_tweets_labeled*). Did it work as expected ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462ba09",
   "metadata": {},
   "source": [
    "### III - 4 Take away\n",
    "\n",
    "**Idea**: the key to improving representations is to embed data capturing text statistics in a compact space.\n",
    "\n",
    "But how ? \n",
    "Let's look at how a compact **modern (deep learning based) model** can better capture what's happening in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cmarkea/distilcamembert-base\")\n",
    "model = AutoModel.from_pretrained(\"cmarkea/distilcamembert-base\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is very inefficient: it will take documents one by one and make them go through the model\n",
    "# We can usually process several of them together to gain time: this is called batching\n",
    "# Batching may require a large quantity of memory, and to avoid any issue when running this locally,\n",
    "# we will keep this (very slow and) inefficient solution. \n",
    "vectors = []\n",
    "for i, example in enumerate(AS13_df['description'].tolist()):\n",
    "    inputs = tokenizer(example, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    vectors.append(outputs.last_hidden_state[0,0,:].detach().numpy()[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model outputs vectors of size 768\n",
    "cam_rep = np.concatenate(vectors, axis=0)\n",
    "print(cam_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8184611",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                    init='random', metric='cosine',\n",
    "                    perplexity=50.0, square_distances=True).fit_transform(cam_rep)\n",
    "print(docs_tsne_th.shape)\n",
    "\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': AS13_df['description'],\n",
    "                        'Category': AS13_df['labelGeneric']})\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='Category',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bd85c",
   "metadata": {},
   "source": [
    "We will see how such a model (*CamemBERT*) works in a few months ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
